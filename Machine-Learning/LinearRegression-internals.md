
# Linear Regression Internals Explained

---

## ðŸ”¥ How `LinearRegression()` Works Internally

### 1. Problem Setup

- **Input (X)**: Feature(s) (e.g., house size)
- **Output (y)**: Target (e.g., house price)

Trying to find the best line:

\[ \text{Predicted value (\hat{y})} = wX + b \]

Where:
- `w`: weight (slope)
- `b`: bias (intercept)

---

### 2. Goal

Minimize the error (difference between actual `y` and predicted `\hat{y}`) using **Mean Squared Error (MSE)**:

\[ \text{MSE} = \frac{1}{n} \sum_{i=1}^{n}(y_i - (wX_i + b))^2 \]

---

### 3. How scikit-learn Finds `w` and `b`

It uses **Least Squares Method** (Analytical solution), not gradient descent:

### The Formula:

\[ w = (X^TX)^{-1}X^Ty \]

Where:
- \(X\) = input matrix
- \(X^T\) = transpose of \(X\)
- \((X^TX)^{-1}\) = inverse of \(X^TX\)
- \(y\) = output vector

**Bias** \(b\) is also automatically calculated if `fit_intercept=True`.

---

### 4. Steps Inside `model.fit(X, y)`

- Add a column of ones to `X` to account for `b`
- Solve using the normal equation
- Save learned parameters (`model.coef_`, `model.intercept_`)

---

### 5. Steps Inside `model.predict(X_new)`

- For a new input `X_new`, predict:

\[ \hat{y} = wX\_new + b \]


---

# ðŸŒŸ Summary Table

| Step | What Happens |
|:---|:---|
| `model.fit(X, y)` | Finds best `w` and `b` |
| `model.predict(X_new)` | Predicts using `wX + b` |

---

# ðŸ“¦ How sklearn Does it Internally (Code Sketch)

```python
# Add column for bias
X_b = np.c_[np.ones((X.shape[0], 1)), X]

# Solve using normal equation
theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)

# theta_best[0] -> intercept (b)
# theta_best[1:] -> coefficient/weight (w)
```

---

# ðŸš€ Your Example (House Size vs Price)

- Increase 500 sq ft -> Increase 25 lakhs
- So slope `w â‰ˆ 0.05`
- Intercept `b â‰ˆ 0`

Prediction for 1800:

\[ \text{price} = (0.05 \times 1800) = 90 \text{ lakhs} \]

---

# ðŸ’¡ Final Notes

- sklearn `LinearRegression()` uses **closed-form solution**, not **gradient descent**.
- For massive datasets, it may use optimized solvers internally.

---

# ðŸ’¡ Manual Implementation Using Only NumPy

Let's implement our own `LinearRegression`!

```python
import numpy as np

# Data
X = np.array([[1000], [1500], [2000], [2500]])
y = np.array([50, 75, 100, 125])

# Add bias term
X_b = np.c_[np.ones((X.shape[0], 1)), X]

# Solve for best theta
theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)

# Intercept and coefficient
b = theta_best[0]
w = theta_best[1]

print(f"Intercept (b): {b}")
print(f"Weight (w): {w}")

# Predict for 1800
X_new = np.array([[1, 1800]])  # include bias
prediction = X_new.dot(theta_best)
print(f"Predicted price for 1800 sq ft: {prediction[0]} lakhs")
```

**Output:**
```text
Intercept (b): ~0.0
Weight (w): 0.05
Predicted price for 1800 sq ft: 90.0 lakhs
```
